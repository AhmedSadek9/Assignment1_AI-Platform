{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a45697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " COMPUTATIONAL GRAPH WITH PyTorch - COMPLETE IMPLEMENTATION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Building a Computational Graph with PyTorch - Complete Implementation in One Cell\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "print(\" COMPUTATIONAL GRAPH WITH PyTorch - COMPLETE IMPLEMENTATION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Class\n",
    "class CustomNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNeuralNet, self).__init__()\n",
    "        \n",
    "        print(\"Initializing Neural Network Parameters...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Layer 1: 3 neurons\n",
    "        self.w00 = torch.tensor(0.5, requires_grad=True)\n",
    "        self.b00 = torch.tensor(0.2, requires_grad=True)\n",
    "        self.w01 = torch.tensor(-0.3, requires_grad=True)\n",
    "        self.b01 = torch.tensor(0.1, requires_grad=True)\n",
    "        self.w02 = torch.tensor(0.8, requires_grad=True)\n",
    "        self.b02 = torch.tensor(-0.1, requires_grad=True)\n",
    "        \n",
    "        # Layer 2: 2 neurons\n",
    "        self.w10 = torch.tensor(0.7, requires_grad=True)\n",
    "        self.b10 = torch.tensor(0.05, requires_grad=True)\n",
    "        self.w11 = torch.tensor(-0.4, requires_grad=True)\n",
    "        self.b11 = torch.tensor(0.15, requires_grad=True)\n",
    "        \n",
    "        # Output Layer: 1 neuron\n",
    "        self.w20 = torch.tensor(0.6, requires_grad=True)\n",
    "        self.b20 = torch.tensor(0.0, requires_grad=True)\n",
    "        \n",
    "        # Print initial parameters\n",
    "        print(\"Layer 1 Parameters:\")\n",
    "        print(f\"  Neuron 0: w00={self.w00.item()}, b00={self.b00.item()}\")\n",
    "        print(f\"  Neuron 1: w01={self.w01.item()}, b01={self.b01.item()}\")\n",
    "        print(f\"  Neuron 2: w02={self.w02.item()}, b02={self.b02.item()}\")\n",
    "        print(\"\\nLayer 2 Parameters:\")\n",
    "        print(f\"  Neuron 0: w10={self.w10.item()}, b10={self.b10.item()}\")\n",
    "        print(f\"  Neuron 1: w11={self.w11.item()}, b11={self.b11.item()}\")\n",
    "        print(\"\\nOutput Layer Parameters:\")\n",
    "        print(f\"  Neuron 0: w20={self.w20.item()}, b20={self.b20.item()}\")\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(\" STARTING FORWARD PASS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Layer 1: 3 neurons with ReLU activation\n",
    "        print(\"\\n LAYER 1 - 3 Neurons with ReLU Activation\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Neuron 0\n",
    "        neuron00 = self.w00 * x + self.b00\n",
    "        relu00 = torch.relu(neuron00)\n",
    "        print(f\"Neuron 0: {self.w00.item():.1f}*{x.item():.1f} + {self.b00.item():.1f} = {neuron00.item():.3f}\")\n",
    "        print(f\"        ReLU({neuron00.item():.3f}) = {relu00.item():.3f}\")\n",
    "        \n",
    "        # Neuron 1\n",
    "        neuron01 = self.w01 * x + self.b01\n",
    "        relu01 = torch.relu(neuron01)\n",
    "        print(f\"Neuron 1: {self.w01.item():.1f}*{x.item():.1f} + {self.b01.item():.1f} = {neuron01.item():.3f}\")\n",
    "        print(f\"         ReLU({neuron01.item():.3f}) = {relu01.item():.3f}\")\n",
    "        \n",
    "        # Neuron 2\n",
    "        neuron02 = self.w02 * x + self.b02\n",
    "        relu02 = torch.relu(neuron02)\n",
    "        print(f\"Neuron 2: {self.w02.item():.1f}*{x.item():.1f} + {self.b02.item():.1f} = {neuron02.item():.3f}\")\n",
    "        print(f\"         ReLU({neuron02.item():.3f}) = {relu02.item():.3f}\")\n",
    "        \n",
    "        # Layer 2: 2 neurons with Sigmoid activation\n",
    "        print(\"\\n LAYER 2 - 2 Neurons with Sigmoid Activation\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Neuron 0\n",
    "        neuron10 = self.w10 * relu00 + self.b10\n",
    "        sigmoid10 = torch.sigmoid(neuron10)\n",
    "        print(f\"Neuron 0: {self.w10.item():.1f}*{relu00.item():.3f} + {self.b10.item():.2f} = {neuron10.item():.3f}\")\n",
    "        print(f\"         Sigmoid({neuron10.item():.3f}) = {sigmoid10.item():.3f}\")\n",
    "        \n",
    "        # Neuron 1\n",
    "        neuron11 = self.w11 * relu01 + self.b11\n",
    "        sigmoid11 = torch.sigmoid(neuron11)\n",
    "        print(f\"Neuron 1: {self.w11.item():.1f}*{relu01.item():.3f} + {self.b11.item():.2f} = {neuron11.item():.3f}\")\n",
    "        print(f\"         Sigmoid({neuron11.item():.3f}) = {sigmoid11.item():.3f}\")\n",
    "        \n",
    "        # Combine outputs with Tanh\n",
    "        print(\"\\n COMBINE OUTPUTS with Tanh Activation\")\n",
    "        print(\"-\" * 40)\n",
    "        combined = sigmoid10 + sigmoid11\n",
    "        tanh_output = torch.tanh(combined)\n",
    "        print(f\"Combined: {sigmoid10.item():.3f} + {sigmoid11.item():.3f} = {combined.item():.3f}\")\n",
    "        print(f\"Tanh({combined.item():.3f}) = {tanh_output.item():.3f}\")\n",
    "        \n",
    "        # Output layer (linear activation)\n",
    "        print(\"\\n OUTPUT LAYER - Linear Activation\")\n",
    "        print(\"-\" * 40)\n",
    "        final_output = self.w20 * tanh_output + self.b20\n",
    "        print(f\"Output: {self.w20.item():.1f}*{tanh_output.item():.3f} + {self.b20.item():.1f} = {final_output.item():.3f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" FORWARD PASS COMPLETED\")\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62eaed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CREATING NEURAL NETWORK MODEL\n",
      "======================================================================\n",
      "Initializing Neural Network Parameters...\n",
      "--------------------------------------------------\n",
      "Layer 1 Parameters:\n",
      "  Neuron 0: w00=0.5, b00=0.20000000298023224\n",
      "  Neuron 1: w01=-0.30000001192092896, b01=0.10000000149011612\n",
      "  Neuron 2: w02=0.800000011920929, b02=-0.10000000149011612\n",
      "\n",
      "Layer 2 Parameters:\n",
      "  Neuron 0: w10=0.699999988079071, b10=0.05000000074505806\n",
      "  Neuron 1: w11=-0.4000000059604645, b11=0.15000000596046448\n",
      "\n",
      "Output Layer Parameters:\n",
      "  Neuron 0: w20=0.6000000238418579, b20=0.0\n",
      "\n",
      "==================================================\n",
      "Input tensor: x = 1.0\n",
      "Requires gradient: True\n",
      "\n",
      " PERFORMING FORWARD PASS\n",
      "======================================================================\n",
      " STARTING FORWARD PASS\n",
      "==================================================\n",
      "\n",
      " LAYER 1 - 3 Neurons with ReLU Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.5*1.0 + 0.2 = 0.700\n",
      "         ReLU(0.700) = 0.700\n",
      "Neuron 1: -0.3*1.0 + 0.1 = -0.200\n",
      "         ReLU(-0.200) = 0.000\n",
      "Neuron 2: 0.8*1.0 + -0.1 = 0.700\n",
      "         ReLU(0.700) = 0.700\n",
      "\n",
      " LAYER 2 - 2 Neurons with Sigmoid Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.7*0.700 + 0.05 = 0.540\n",
      "         Sigmoid(0.540) = 0.632\n",
      "Neuron 1: -0.4*0.000 + 0.15 = 0.150\n",
      "         Sigmoid(0.150) = 0.537\n",
      "\n",
      " COMBINE OUTPUTS with Tanh Activation\n",
      "----------------------------------------\n",
      "Combined: 0.632 + 0.537 = 1.169\n",
      "Tanh(1.169) = 0.824\n",
      "\n",
      " OUTPUT LAYER - Linear Activation\n",
      "----------------------------------------\n",
      "Output: 0.6*0.824 + 0.0 = 0.494\n",
      "\n",
      "==================================================\n",
      " FORWARD PASS COMPLETED\n",
      "\n",
      " FINAL OUTPUT: 0.494417\n",
      "\n",
      " CALCULATING GRADIENTS WITH BACKPROPAGATION\n",
      "======================================================================\n",
      "Gradients calculated successfully!\n",
      "\n",
      " GRADIENT VALUES:\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network model\n",
    "print(\" CREATING NEURAL NETWORK MODEL\")\n",
    "print(\"=\" * 70)\n",
    "model = CustomNeuralNet()\n",
    "\n",
    "# Define input\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "print(f\"Input tensor: x = {x.item()}\")\n",
    "print(f\"Requires gradient: {x.requires_grad}\")\n",
    "\n",
    "# Perform forward pass\n",
    "print(\"\\n\" + \" PERFORMING FORWARD PASS\")\n",
    "print(\"=\" * 70)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"\\n FINAL OUTPUT: {output.item():.6f}\")\n",
    "\n",
    "# Calculate gradients using backpropagation\n",
    "print(\"\\n\" + \" CALCULATING GRADIENTS WITH BACKPROPAGATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reset gradients first\n",
    "for param in [model.w00, model.b00, model.w01, model.b01, model.w02, model.b02, \n",
    "              model.w10, model.b10, model.w11, model.b11, model.w20, model.b20]:\n",
    "    if param.grad is not None:\n",
    "        param.grad.zero_()\n",
    "\n",
    "if x.grad is not None:\n",
    "    x.grad.zero_()\n",
    "\n",
    "# Perform backward pass\n",
    "output.backward()\n",
    "\n",
    "print(\"Gradients calculated successfully!\")\n",
    "print(\"\\n GRADIENT VALUES:\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeae4f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Gradient:\n",
      "  ∂output/∂x: 0.015680\n",
      "\n",
      "Layer 1 Gradients:\n",
      "  ∂output/∂w00: 0.031360\n",
      "  ∂output/∂b00: 0.031360\n",
      "  ∂output/∂w01: 0.000000\n",
      "  ∂output/∂b01: 0.000000\n",
      "  ∂output/∂w02: 0.000000 (No gradient flow)\n",
      "  ∂output/∂b02: 0.000000 (No gradient flow)\n",
      "\n",
      "Layer 2 Gradients:\n",
      "  ∂output/∂w10: 0.031360\n",
      "  ∂output/∂b10: 0.044800\n",
      "  ∂output/∂w11: 0.000000\n",
      "  ∂output/∂b11: 0.047877\n",
      "\n",
      "Output Layer Gradients:\n",
      "  ∂output/∂w20: 0.824029\n",
      "  ∂output/∂b20: 1.000000\n",
      "\n",
      " NETWORK ARCHITECTURE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Computational Graph Structure:\n",
      "\n",
      "Input (x)                             \n",
      "    │                                 \n",
      "    ├─ Layer 1 (3 neurons)            \n",
      "    │  ├─ Neuron 0: w00*x + b00 → ReLU\n",
      "    │  ├─ Neuron 1: w01*x + b01 → ReLU  \n",
      "    │  └─ Neuron 2: w02*x + b02 → ReLU\n",
      "    │                                 \n",
      "    ├─ Layer 2 (2 neurons)            \n",
      "    │  ├─ Neuron 0: w10*relu00 + b10 → Sigmoid\n",
      "    │  └─ Neuron 1: w11*relu01 + b11 → Sigmoid\n",
      "    │                                 \n",
      "    ├─ Combine: sum → Tanh            \n",
      "    │                                 \n",
      "    └─ Output: w20*tanh + b20 → Linear\n",
      "\n",
      "Activation Functions:\n",
      "• ReLU: max(0, x)\n",
      "• Sigmoid: 1/(1 + e^(-x))  \n",
      "• Tanh: (e^x - e^(-x))/(e^x + e^(-x))\n",
      "• Linear: no activation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print gradients for all parameters with safe handling\n",
    "def safe_grad_print(grad_tensor, name):\n",
    "    if grad_tensor.grad is not None:\n",
    "        return f\"{grad_tensor.grad.item():.6f}\"\n",
    "    else:\n",
    "        return \"0.000000 (No gradient flow)\"\n",
    "\n",
    "print(\"Input Gradient:\")\n",
    "print(f\"  ∂output/∂x: {safe_grad_print(x, 'x')}\")\n",
    "\n",
    "print(\"\\nLayer 1 Gradients:\")\n",
    "print(f\"  ∂output/∂w00: {safe_grad_print(model.w00, 'w00')}\")\n",
    "print(f\"  ∂output/∂b00: {safe_grad_print(model.b00, 'b00')}\")\n",
    "print(f\"  ∂output/∂w01: {safe_grad_print(model.w01, 'w01')}\")\n",
    "print(f\"  ∂output/∂b01: {safe_grad_print(model.b01, 'b01')}\")\n",
    "print(f\"  ∂output/∂w02: {safe_grad_print(model.w02, 'w02')}\")\n",
    "print(f\"  ∂output/∂b02: {safe_grad_print(model.b02, 'b02')}\")\n",
    "\n",
    "print(\"\\nLayer 2 Gradients:\")\n",
    "print(f\"  ∂output/∂w10: {safe_grad_print(model.w10, 'w10')}\")\n",
    "print(f\"  ∂output/∂b10: {safe_grad_print(model.b10, 'b10')}\")\n",
    "print(f\"  ∂output/∂w11: {safe_grad_print(model.w11, 'w11')}\")\n",
    "print(f\"  ∂output/∂b11: {safe_grad_print(model.b11, 'b11')}\")\n",
    "\n",
    "print(\"\\nOutput Layer Gradients:\")\n",
    "print(f\"  ∂output/∂w20: {safe_grad_print(model.w20, 'w20')}\")\n",
    "print(f\"  ∂output/∂b20: {safe_grad_print(model.b20, 'b20')}\")\n",
    "\n",
    "# Network Architecture Visualization\n",
    "print(\"\\n\" + \" NETWORK ARCHITECTURE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "architecture = \"\"\"\n",
    "Computational Graph Structure:\n",
    "                                    \n",
    "Input (x)                             \n",
    "    │                                 \n",
    "    ├─ Layer 1 (3 neurons)            \n",
    "    │  ├─ Neuron 0: w00*x + b00 → ReLU\n",
    "    │  ├─ Neuron 1: w01*x + b01 → ReLU  \n",
    "    │  └─ Neuron 2: w02*x + b02 → ReLU\n",
    "    │                                 \n",
    "    ├─ Layer 2 (2 neurons)            \n",
    "    │  ├─ Neuron 0: w10*relu00 + b10 → Sigmoid\n",
    "    │  └─ Neuron 1: w11*relu01 + b11 → Sigmoid\n",
    "    │                                 \n",
    "    ├─ Combine: sum → Tanh            \n",
    "    │                                 \n",
    "    └─ Output: w20*tanh + b20 → Linear\n",
    "                                    \n",
    "Activation Functions:\n",
    "• ReLU: max(0, x)\n",
    "• Sigmoid: 1/(1 + e^(-x))  \n",
    "• Tanh: (e^x - e^(-x))/(e^x + e^(-x))\n",
    "• Linear: no activation\n",
    "\"\"\"\n",
    "\n",
    "print(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6d2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TESTING WITH DIFFERENT INPUTS\n",
      "======================================================================\n",
      "Input\t\tOutput\n",
      "-------------------------\n",
      "Initializing Neural Network Parameters...\n",
      "--------------------------------------------------\n",
      "Layer 1 Parameters:\n",
      "  Neuron 0: w00=0.5, b00=0.20000000298023224\n",
      "  Neuron 1: w01=-0.30000001192092896, b01=0.10000000149011612\n",
      "  Neuron 2: w02=0.800000011920929, b02=-0.10000000149011612\n",
      "\n",
      "Layer 2 Parameters:\n",
      "  Neuron 0: w10=0.699999988079071, b10=0.05000000074505806\n",
      "  Neuron 1: w11=-0.4000000059604645, b11=0.15000000596046448\n",
      "\n",
      "Output Layer Parameters:\n",
      "  Neuron 0: w20=0.6000000238418579, b20=0.0\n",
      "\n",
      "==================================================\n",
      " STARTING FORWARD PASS\n",
      "==================================================\n",
      "\n",
      " LAYER 1 - 3 Neurons with ReLU Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.5*-2.0 + 0.2 = -0.800\n",
      "         ReLU(-0.800) = 0.000\n",
      "Neuron 1: -0.3*-2.0 + 0.1 = 0.700\n",
      "         ReLU(0.700) = 0.700\n",
      "Neuron 2: 0.8*-2.0 + -0.1 = -1.700\n",
      "         ReLU(-1.700) = 0.000\n",
      "\n",
      " LAYER 2 - 2 Neurons with Sigmoid Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.7*0.000 + 0.05 = 0.050\n",
      "         Sigmoid(0.050) = 0.512\n",
      "Neuron 1: -0.4*0.700 + 0.15 = -0.130\n",
      "         Sigmoid(-0.130) = 0.468\n",
      "\n",
      " COMBINE OUTPUTS with Tanh Activation\n",
      "----------------------------------------\n",
      "Combined: 0.512 + 0.468 = 0.980\n",
      "Tanh(0.980) = 0.753\n",
      "\n",
      " OUTPUT LAYER - Linear Activation\n",
      "----------------------------------------\n",
      "Output: 0.6*0.753 + 0.0 = 0.452\n",
      "\n",
      "==================================================\n",
      " FORWARD PASS COMPLETED\n",
      "  -2.0\t\t  0.4519\n",
      " STARTING FORWARD PASS\n",
      "==================================================\n",
      "\n",
      " LAYER 1 - 3 Neurons with ReLU Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.5*-1.0 + 0.2 = -0.300\n",
      "         ReLU(-0.300) = 0.000\n",
      "Neuron 1: -0.3*-1.0 + 0.1 = 0.400\n",
      "         ReLU(0.400) = 0.400\n",
      "Neuron 2: 0.8*-1.0 + -0.1 = -0.900\n",
      "         ReLU(-0.900) = 0.000\n",
      "\n",
      " LAYER 2 - 2 Neurons with Sigmoid Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.7*0.000 + 0.05 = 0.050\n",
      "         Sigmoid(0.050) = 0.512\n",
      "Neuron 1: -0.4*0.400 + 0.15 = -0.010\n",
      "         Sigmoid(-0.010) = 0.498\n",
      "\n",
      " COMBINE OUTPUTS with Tanh Activation\n",
      "----------------------------------------\n",
      "Combined: 0.512 + 0.498 = 1.010\n",
      "Tanh(1.010) = 0.766\n",
      "\n",
      " OUTPUT LAYER - Linear Activation\n",
      "----------------------------------------\n",
      "Output: 0.6*0.766 + 0.0 = 0.459\n",
      "\n",
      "==================================================\n",
      " FORWARD PASS COMPLETED\n",
      "  -1.0\t\t  0.4595\n",
      " STARTING FORWARD PASS\n",
      "==================================================\n",
      "\n",
      " LAYER 1 - 3 Neurons with ReLU Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.5*0.0 + 0.2 = 0.200\n",
      "         ReLU(0.200) = 0.200\n",
      "Neuron 1: -0.3*0.0 + 0.1 = 0.100\n",
      "         ReLU(0.100) = 0.100\n",
      "Neuron 2: 0.8*0.0 + -0.1 = -0.100\n",
      "         ReLU(-0.100) = 0.000\n",
      "\n",
      " LAYER 2 - 2 Neurons with Sigmoid Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.7*0.200 + 0.05 = 0.190\n",
      "         Sigmoid(0.190) = 0.547\n",
      "Neuron 1: -0.4*0.100 + 0.15 = 0.110\n",
      "         Sigmoid(0.110) = 0.527\n",
      "\n",
      " COMBINE OUTPUTS with Tanh Activation\n",
      "----------------------------------------\n",
      "Combined: 0.547 + 0.527 = 1.075\n",
      "Tanh(1.075) = 0.791\n",
      "\n",
      " OUTPUT LAYER - Linear Activation\n",
      "----------------------------------------\n",
      "Output: 0.6*0.791 + 0.0 = 0.475\n",
      "\n",
      "==================================================\n",
      " FORWARD PASS COMPLETED\n",
      "   0.0\t\t  0.4748\n",
      " STARTING FORWARD PASS\n",
      "==================================================\n",
      "\n",
      " LAYER 1 - 3 Neurons with ReLU Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.5*0.5 + 0.2 = 0.450\n",
      "         ReLU(0.450) = 0.450\n",
      "Neuron 1: -0.3*0.5 + 0.1 = -0.050\n",
      "         ReLU(-0.050) = 0.000\n",
      "Neuron 2: 0.8*0.5 + -0.1 = 0.300\n",
      "         ReLU(0.300) = 0.300\n",
      "\n",
      " LAYER 2 - 2 Neurons with Sigmoid Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.7*0.450 + 0.05 = 0.365\n",
      "         Sigmoid(0.365) = 0.590\n",
      "Neuron 1: -0.4*0.000 + 0.15 = 0.150\n",
      "         Sigmoid(0.150) = 0.537\n",
      "\n",
      " COMBINE OUTPUTS with Tanh Activation\n",
      "----------------------------------------\n",
      "Combined: 0.590 + 0.537 = 1.128\n",
      "Tanh(1.128) = 0.810\n",
      "\n",
      " OUTPUT LAYER - Linear Activation\n",
      "----------------------------------------\n",
      "Output: 0.6*0.810 + 0.0 = 0.486\n",
      "\n",
      "==================================================\n",
      " FORWARD PASS COMPLETED\n",
      "   0.5\t\t  0.4861\n",
      " STARTING FORWARD PASS\n",
      "==================================================\n",
      "\n",
      " LAYER 1 - 3 Neurons with ReLU Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.5*1.0 + 0.2 = 0.700\n",
      "         ReLU(0.700) = 0.700\n",
      "Neuron 1: -0.3*1.0 + 0.1 = -0.200\n",
      "         ReLU(-0.200) = 0.000\n",
      "Neuron 2: 0.8*1.0 + -0.1 = 0.700\n",
      "         ReLU(0.700) = 0.700\n",
      "\n",
      " LAYER 2 - 2 Neurons with Sigmoid Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.7*0.700 + 0.05 = 0.540\n",
      "         Sigmoid(0.540) = 0.632\n",
      "Neuron 1: -0.4*0.000 + 0.15 = 0.150\n",
      "         Sigmoid(0.150) = 0.537\n",
      "\n",
      " COMBINE OUTPUTS with Tanh Activation\n",
      "----------------------------------------\n",
      "Combined: 0.632 + 0.537 = 1.169\n",
      "Tanh(1.169) = 0.824\n",
      "\n",
      " OUTPUT LAYER - Linear Activation\n",
      "----------------------------------------\n",
      "Output: 0.6*0.824 + 0.0 = 0.494\n",
      "\n",
      "==================================================\n",
      " FORWARD PASS COMPLETED\n",
      "   1.0\t\t  0.4944\n",
      " STARTING FORWARD PASS\n",
      "==================================================\n",
      "\n",
      " LAYER 1 - 3 Neurons with ReLU Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.5*2.0 + 0.2 = 1.200\n",
      "         ReLU(1.200) = 1.200\n",
      "Neuron 1: -0.3*2.0 + 0.1 = -0.500\n",
      "         ReLU(-0.500) = 0.000\n",
      "Neuron 2: 0.8*2.0 + -0.1 = 1.500\n",
      "         ReLU(1.500) = 1.500\n",
      "\n",
      " LAYER 2 - 2 Neurons with Sigmoid Activation\n",
      "----------------------------------------\n",
      "Neuron 0: 0.7*1.200 + 0.05 = 0.890\n",
      "         Sigmoid(0.890) = 0.709\n",
      "Neuron 1: -0.4*0.000 + 0.15 = 0.150\n",
      "         Sigmoid(0.150) = 0.537\n",
      "\n",
      " COMBINE OUTPUTS with Tanh Activation\n",
      "----------------------------------------\n",
      "Combined: 0.709 + 0.537 = 1.246\n",
      "Tanh(1.246) = 0.847\n",
      "\n",
      " OUTPUT LAYER - Linear Activation\n",
      "----------------------------------------\n",
      "Output: 0.6*0.847 + 0.0 = 0.508\n",
      "\n",
      "==================================================\n",
      " FORWARD PASS COMPLETED\n",
      "   2.0\t\t  0.5083\n",
      "\n",
      " GRADIENT EXPLANATION\n",
      "======================================================================\n",
      "Why some gradients are zero:\n",
      "• Neuron 1 in Layer 1 has ReLU output = 0.0 (negative input)\n",
      "• This breaks the gradient flow for w01, b01, w11, b11\n",
      "• This is expected behavior in neural networks with ReLU\n",
      "\n",
      "Gradient flow follows the path:\n",
      "Output → w20/b20 → Tanh → Combined → Sigmoid10/Sigmoid11 → ReLU00/ReLU01 → ...\n"
     ]
    }
   ],
   "source": [
    "# Test with Different Inputs\n",
    "print(\"\\n\" + \" TESTING WITH DIFFERENT INPUTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_inputs = [-2.0, -1.0, 0.0, 0.5, 1.0, 2.0]\n",
    "\n",
    "print(\"Input\\t\\tOutput\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Create a new model for testing to avoid gradient issues\n",
    "test_model = CustomNeuralNet()\n",
    "for test_x in test_inputs:\n",
    "    x_test = torch.tensor(test_x)\n",
    "    with torch.no_grad():  \n",
    "        output_test = test_model.forward(x_test)\n",
    "    print(f\"{test_x:>6.1f}\\t\\t{output_test.item():>8.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a272cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
